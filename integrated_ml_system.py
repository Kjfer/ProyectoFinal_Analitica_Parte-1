# integrated_ml_system.py
"""
Sistema integrado que combina el an√°lisis de ML con el chatbot de WhatsApp
para DryWall Alert con detecci√≥n inteligente de anomal√≠as

Este m√≥dulo implementa la integraci√≥n en tiempo real del sistema:
1. Carga modelos de ML entrenados previamente
2. Recibe datos en tiempo real del sensor de humedad
3. Aplica algoritmos de ML para detectar anomal√≠as
4. Genera alertas inteligentes con an√°lisis de confianza
5. Se integra con el sistema de mensajer√≠a WhatsApp

El objetivo es reemplazar alertas simples por umbral con un sistema
inteligente que reduce falsas alarmas y mejora la detecci√≥n.
"""

# Librer√≠as para an√°lisis de datos y machine learning
import pandas as pd  # Manipulaci√≥n de datos estructurados
import numpy as np   # Operaciones matem√°ticas y arrays
import pickle        # Serializaci√≥n para guardar/cargar modelos entrenados
import time          # Control de tiempo para monitoreo continuo
from datetime import datetime  # Manejo de timestamps y fechas
import os            # Interacci√≥n con sistema operativo (variables de entorno)

# Algoritmos de ML espec√≠ficos utilizados en el sistema integrado
from sklearn.ensemble import RandomForestClassifier, IsolationForest
# - RandomForest: Mejor modelo supervisado seg√∫n an√°lisis comparativo
# - IsolationForest: Mejor modelo de detecci√≥n de anomal√≠as no supervisado

from sklearn.preprocessing import StandardScaler  # Normalizaci√≥n de datos
from sklearn.model_selection import train_test_split  # Divisi√≥n de datos
import warnings
warnings.filterwarnings('ignore')  # Suprimir advertencias para output limpio

class SmartDryWallDetector:
    """
    Detector inteligente de filtraciones usando Machine Learning en tiempo real.
    
    Esta clase encapsula toda la l√≥gica de detecci√≥n inteligente:
    - Entrena modelos ML con datos hist√≥ricos
    - Guarda/carga modelos para uso persistente
    - Analiza lecturas en tiempo real del sensor
    - Combina m√∫ltiples algoritmos para mayor precisi√≥n
    - Genera alertas contextualizadas con niveles de confianza
    
    Flujo de funcionamiento:
    1. Entrenamiento inicial con datos hist√≥ricos (train_models)
    2. Persistencia de modelos entrenados (save/load_models)
    3. An√°lisis en tiempo real de cada lectura (predict_anomaly)
    4. Generaci√≥n de alertas inteligentes (generate_alert_message)
    5. Monitoreo continuo integrado (continuous_monitoring)
    
    Algoritmos utilizados:
    - RandomForest: Clasificaci√≥n supervisada principal
    - IsolationForest: Detecci√≥n de anomal√≠as no supervisada
    - Combinaci√≥n h√≠brida para mayor robustez
    """
    
    def __init__(self, data_file='synthetic_drywall_data_7days.csv'):
        """
        Inicializa el detector inteligente de filtraciones.
        
        ACTUALIZADO: Adaptado para el nuevo dataset sint√©tico de 7 d√≠as con
        caracter√≠sticas expandidas que mejoran significativamente la detecci√≥n.
        
        Args:
            data_file (str): Archivo CSV con datos hist√≥ricos del sensor
                           Por defecto usa el nuevo dataset de 7 d√≠as
            
        Atributos:
            data_file: Ruta a datos hist√≥ricos para entrenamiento
            model: Clasificador Random Forest (modelo supervisado principal)
            anomaly_detector: Isolation Forest (detector de anomal√≠as no supervisado)
            scaler: Normalizador de caracter√≠sticas para consistencia
            is_trained: Flag que indica si los modelos est√°n entrenados
            threshold_basic: Umbral b√°sico de respaldo (50% humedad)
            feature_columns: Lista de caracter√≠sticas para predicci√≥n
        """
        self.data_file = data_file
        
        # Modelos de Machine Learning
        self.model = None              # Random Forest para clasificaci√≥n
        self.anomaly_detector = None   # Isolation Forest para anomal√≠as
        self.scaler = StandardScaler() # Normalizador de datos
        
        # Estado del sistema
        self.is_trained = False        # ¬øModelos entrenados?
        self.threshold_basic = 50      # Umbral de respaldo (humedad %)
        
        # NUEVO: Caracter√≠sticas expandidas del dataset sint√©tico
        self.feature_columns = [
            'humidity_pct',           # Humedad principal
            'raw_value',             # Valor crudo del sensor  
            'raw_normalized',        # Valor raw normalizado
            'hour',                  # Hora del d√≠a
            'day_of_week',          # D√≠a de semana
            'is_weekend',           # ¬øEs fin de semana?
            'is_night',             # ¬øEs horario nocturno?
            'humidity_category',     # Categor√≠a de humedad
            'humidity_risk_level',   # Nivel de riesgo calculado
            'sensor_stability',      # Estabilidad del sensor
            'humidity_change',       # Cambio en humedad
            'raw_change'            # Cambio en valor raw
        ]
        
        print("ü§ñ Smart DryWall Detector inicializado (Dataset 7 d√≠as)")
        print(f"üìÇ Datos de entrenamiento: {data_file}")
        print(f"‚öôÔ∏è Caracter√≠sticas ML: {len(self.feature_columns)}")
        print(f"‚öôÔ∏è Umbral b√°sico de respaldo: {self.threshold_basic}%")
        
    def train_models(self):
        """
        Entrena los modelos de ML con datos hist√≥ricos del sensor.
        
        ACTUALIZADO: Aprovecha las nuevas caracter√≠sticas del dataset sint√©tico
        de 7 d√≠as para entrenar modelos m√°s precisos y robustos.
        
        Este m√©todo implementa el pipeline completo de entrenamiento:
        1. Carga datos hist√≥ricos enriquecidos (10,080 registros)
        2. Utiliza caracter√≠sticas pre-calculadas del dataset sint√©tico
        3. Entrena modelos con caracter√≠sticas expandidas (12 features)
        4. Eval√∫a rendimiento en datos de prueba
        5. Persiste modelos para uso futuro
        
        Ventajas del nuevo dataset:
        - 13x m√°s datos (10,080 vs ~800 registros)
        - 3x m√°s caracter√≠sticas (12 vs 4 features)
        - Variables objetivo ya calculadas
        - Caracter√≠sticas temporales y contextuales avanzadas
        - Datos sint√©ticos balanceados y representativos
        """
        print("üß† Entrenando modelos ML con dataset sint√©tico de 7 d√≠as...")
        print("üìä Cargando datos hist√≥ricos enriquecidos...")
        
        # Cargar y preparar datos enriquecidos
        df = pd.read_csv(self.data_file)
        print(f"‚úÖ {len(df):,} registros hist√≥ricos cargados (7 d√≠as)")
        print(f"üìÖ Periodo: {pd.to_datetime(df['timestamp']).dt.date.min()} a {pd.to_datetime(df['timestamp']).dt.date.max()}")
        
        # Verificar integridad del dataset
        print(f"\nüîç VERIFICACI√ìN DE INTEGRIDAD:")
        print(f"   Columnas disponibles: {len(df.columns)}")
        print(f"   Registros totales: {len(df):,}")
        print(f"   Dispositivos √∫nicos: {df['device_id'].nunique()}")
        
        # An√°lisis de la variable objetivo (ya incluida)
        anomaly_distribution = df['is_anomaly'].value_counts()
        print(f"   Casos normales: {anomaly_distribution[0]:,} ({anomaly_distribution[0]/len(df):.1%})")
        print(f"   Casos an√≥malos: {anomaly_distribution[1]:,} ({anomaly_distribution[1]/len(df):.1%})")
        
        # Verificar disponibilidad de caracter√≠sticas requeridas
        missing_features = [col for col in self.feature_columns if col not in df.columns]
        if missing_features:
            print(f"‚ö†Ô∏è Caracter√≠sticas faltantes: {missing_features}")
            # Crear caracter√≠sticas faltantes si es necesario
            if 'minute' not in df.columns:
                df['minute'] = pd.to_datetime(df['timestamp']).dt.minute
                print("‚úÖ Caracter√≠stica 'minute' creada")
        
        # Preparar datos para entrenamiento
        print(f"\n‚öôÔ∏è PREPARACI√ìN DE CARACTER√çSTICAS:")
        print(f"   Caracter√≠sticas seleccionadas: {len(self.feature_columns)}")
        for i, feature in enumerate(self.feature_columns, 1):
            print(f"   {i:2d}. {feature}")
        
        # Extraer caracter√≠sticas y variable objetivo
        X = df[self.feature_columns]  # Caracter√≠sticas expandidas
        y = df['is_anomaly']         # Variable objetivo ya calculada
        
        # Verificar y manejar valores faltantes
        missing_values = X.isnull().sum()
        if missing_values.any():
            print(f"\n‚ö†Ô∏è VALORES FALTANTES:")
            for col, missing in missing_values[missing_values > 0].items():
                print(f"   {col}: {missing} valores")
            X = X.fillna(X.mean())  # Rellenar con media
            print("‚úÖ Valores faltantes rellenados")
        
        # Normalizaci√≥n de caracter√≠sticas
        X_scaled = self.scaler.fit_transform(X)
        print(f"üìè Caracter√≠sticas normalizadas: {X_scaled.shape}")
        
        # Divisi√≥n estratificada para mantener proporci√≥n de clases
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, 
            test_size=0.3,      # 30% para evaluaci√≥n
            random_state=42,    # Reproducibilidad
            stratify=y          # Mantener proporci√≥n de anomal√≠as
        )
        
        print(f"\nüìö DIVISI√ìN DE DATOS:")
        print(f"   Entrenamiento: {len(X_train):,} muestras")
        print(f"   Evaluaci√≥n: {len(X_test):,} muestras")
        print(f"   Anomal√≠as entrenamiento: {y_train.sum():,} ({y_train.mean():.1%})")
        print(f"   Anomal√≠as evaluaci√≥n: {y_test.sum():,} ({y_test.mean():.1%})")
        
        # ENTRENAMIENTO MODELO 1: Random Forest (Supervisado)
        print(f"\nüå≥ ENTRENANDO RANDOM FOREST (Supervisado)...")
        self.model = RandomForestClassifier(
            n_estimators=150,        # M√°s √°rboles para mayor precisi√≥n
            random_state=42,         # Reproducibilidad
            max_depth=15,           # Profundidad mayor para dataset complejo
            min_samples_split=10,    # Evitar overfitting
            min_samples_leaf=5,      # M√≠nimo en hojas
            class_weight='balanced'  # Balance para clases desbalanceadas
        )
        self.model.fit(X_train, y_train)
        
        # Evaluar Random Forest
        rf_accuracy = self.model.score(X_test, y_test)
        rf_predictions = self.model.predict(X_test)
        rf_precision = np.mean((rf_predictions == 1) & (y_test == 1)) / np.max([np.mean(rf_predictions == 1), 0.001])
        rf_recall = np.mean((rf_predictions == 1) & (y_test == 1)) / np.max([np.mean(y_test == 1), 0.001])
        
        print(f"   ‚úÖ Accuracy: {rf_accuracy:.4f}")
        print(f"   ‚úÖ Precision: {rf_precision:.4f}")
        print(f"   ‚úÖ Recall: {rf_recall:.4f}")
        
        # Importancia de caracter√≠sticas
        feature_importance = pd.DataFrame({
            'feature': self.feature_columns,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print(f"\nüèÜ TOP 5 CARACTER√çSTICAS M√ÅS IMPORTANTES:")
        for i, (_, row) in enumerate(feature_importance.head().iterrows(), 1):
            print(f"   {i}. {row['feature']}: {row['importance']:.4f}")
        
        # ENTRENAMIENTO MODELO 2: Isolation Forest (No supervisado)
        print(f"\nüîç ENTRENANDO ISOLATION FOREST (No supervisado)...")
        self.anomaly_detector = IsolationForest(
            contamination=y_train.mean(),  # Usar tasa real de anomal√≠as
            random_state=42,               # Reproducibilidad
            n_estimators=150,              # M√°s estimadores
            max_samples='auto',            # Muestras autom√°ticas
            bootstrap=True                 # Bootstrap para robustez
        )
        self.anomaly_detector.fit(X_train)
        
        # Evaluar Isolation Forest
        if_predictions = self.anomaly_detector.predict(X_test)
        if_predictions = np.where(if_predictions == -1, 1, 0)  # Convertir -1 a 1
        if_accuracy = np.mean(if_predictions == y_test)
        
        print(f"   ‚úÖ Accuracy: {if_accuracy:.4f}")
        
        # EVALUACI√ìN COMBINADA
        print(f"\nüìä RENDIMIENTO COMBINADO:")
        
        # Consenso entre modelos
        consensus_predictions = (rf_predictions + if_predictions) >= 1
        consensus_accuracy = np.mean(consensus_predictions == y_test)
        
        print(f"   ü§ñ Random Forest solo: {rf_accuracy:.4f}")
        print(f"   üîç Isolation Forest solo: {if_accuracy:.4f}")
        print(f"   üéØ Consenso (OR): {consensus_accuracy:.4f}")
        
        # Marcar como entrenado y guardar
        self.is_trained = True
        self.save_models()
        
        print(f"\n‚úÖ ENTRENAMIENTO COMPLETADO:")
        print(f"   üìà Modelos entrenados con {len(X_train):,} muestras")
        print(f"   üéØ {len(self.feature_columns)} caracter√≠sticas utilizadas")
        print(f"   üíæ Modelos guardados para uso futuro")
        print(f"   üöÄ Sistema listo para detecci√≥n en tiempo real")
        
        # Dividir datos
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.3, random_state=42, stratify=y
        )
        
        # Entrenar Random Forest (mejor modelo supervisado)
        self.model = RandomForestClassifier(n_estimators=100, random_state=42)
        self.model.fit(X_train, y_train)
        
        # Entrenar Isolation Forest (detecci√≥n de anomal√≠as)
        self.anomaly_detector = IsolationForest(contamination=0.1, random_state=42)
        self.anomaly_detector.fit(X_train)
        
        # Evaluar
        accuracy = self.model.score(X_test, y_test)
        print(f"‚úÖ Modelo entrenado con accuracy: {accuracy:.4f}")
        
        self.is_trained = True
        
        # Guardar modelos
        self.save_models()
        
    def save_models(self):
        """
        Guarda los modelos entrenados junto con las caracter√≠sticas utilizadas.
        
        ACTUALIZADO: Incluye las caracter√≠sticas expandidas y metadatos del dataset sint√©tico.
        """
        model_data = {
            'classifier': self.model,
            'anomaly_detector': self.anomaly_detector,
            'scaler': self.scaler,
            'feature_columns': self.feature_columns,  # NUEVO: Guardar caracter√≠sticas
            'is_trained': self.is_trained,
            'dataset_version': 'synthetic_7days',      # NUEVO: Versi√≥n del dataset
            'n_features': len(self.feature_columns),   # NUEVO: N√∫mero de caracter√≠sticas
            'threshold_basic': self.threshold_basic
        }
        
        with open('ml_models_7days.pkl', 'wb') as f:
            pickle.dump(model_data, f)
        print("üíæ Modelos del dataset 7 d√≠as guardados en 'ml_models_7days.pkl'")
        
    def load_models(self):
        """
        Carga modelos pre-entrenados con verificaci√≥n de compatibilidad.
        
        ACTUALIZADO: Verifica compatibilidad con caracter√≠sticas expandidas.
        """
        try:
            # Intentar cargar modelos del dataset 7 d√≠as primero
            if os.path.exists('ml_models_7days.pkl'):
                with open('ml_models_7days.pkl', 'rb') as f:
                    model_data = pickle.load(f)
                    
                self.model = model_data['classifier']
                self.anomaly_detector = model_data['anomaly_detector']
                self.scaler = model_data['scaler']
                
                # Verificar compatibilidad de caracter√≠sticas
                if 'feature_columns' in model_data:
                    loaded_features = model_data['feature_columns']
                    if loaded_features != self.feature_columns:
                        print("‚ö†Ô∏è Caracter√≠sticas del modelo difieren de las actuales")
                        print(f"   Modelo: {len(loaded_features)} caracter√≠sticas")
                        print(f"   Actual: {len(self.feature_columns)} caracter√≠sticas")
                        # Usar caracter√≠sticas del modelo cargado
                        self.feature_columns = loaded_features
                        
                self.is_trained = True
                print("‚úÖ Modelos del dataset 7 d√≠as cargados exitosamente")
                print(f"   üìä Caracter√≠sticas: {len(self.feature_columns)}")
                return True
                
            # Fallback a modelos antiguos si existen
            elif os.path.exists('ml_models.pkl'):
                print("‚ö†Ô∏è Cargando modelos antiguos (compatibilidad limitada)")
                with open('ml_models.pkl', 'rb') as f:
                    model_data = pickle.load(f)
                    
                self.model = model_data['classifier']
                self.anomaly_detector = model_data['anomaly_detector']
                self.scaler = model_data['scaler']
                
                # Usar caracter√≠sticas b√°sicas para compatibilidad
                self.feature_columns = ['humidity_pct', 'raw_value', 'hour', 'minute']
                self.is_trained = True
                print("‚úÖ Modelos antiguos cargados (funcionalidad limitada)")
                return True
                
            else:
                print("‚ö†Ô∏è No se encontraron modelos pre-entrenados")
                return False
                
        except Exception as e:
            print(f"‚ùå Error cargando modelos: {e}")
            return False
            
    def predict_anomaly(self, raw_value, humidity_pct, hour=None, minute=None, 
                       day_of_week=None, is_weekend=None, is_night=None):
        """
        Predice si una lectura del sensor indica anomal√≠a usando ML inteligente.
        
        ACTUALIZADO: Aprovecha las nuevas caracter√≠sticas del dataset sint√©tico
        para predicciones m√°s precisas y contextualizadas.
        
        Este es el coraz√≥n del sistema de detecci√≥n en tiempo real. Combina
        m√∫ltiples enfoques de ML con caracter√≠sticas temporales y contextuales
        avanzadas para una detecci√≥n m√°s robusta y confiable.
        
        Proceso de predicci√≥n mejorado:
        1. Verificar disponibilidad de modelos entrenados
        2. Calcular caracter√≠sticas contextuales autom√°ticamente
        3. Estimar caracter√≠sticas avanzadas (riesgo, estabilidad, cambios)
        4. Ejecutar predicci√≥n con Random Forest (supervisado)
        5. Ejecutar detecci√≥n con Isolation Forest (no supervisado)
        6. Combinar resultados con l√≥gica de consenso mejorada
        7. Calcular nivel de confianza contextualizado
        
        Args:
            raw_value (int): Valor crudo del sensor de humedad
            humidity_pct (float): Porcentaje de humedad calculado
            hour (int, optional): Hora actual (0-23)
            minute (int, optional): Minuto actual (0-59)
            day_of_week (int, optional): D√≠a de semana (0-6)
            is_weekend (bool, optional): ¬øEs fin de semana?
            is_night (bool, optional): ¬øEs horario nocturno?
            
        Returns:
            tuple: (is_anomaly, method, confidence, anomaly_score, context_info)
            - is_anomaly (bool): ¬øSe detect√≥ anomal√≠a?
            - method (str): M√©todo de detecci√≥n utilizado
            - confidence (float): Nivel de confianza (0.0-1.0)
            - anomaly_score (float): Score num√©rico de anomal√≠a
            - context_info (dict): Informaci√≥n contextual de la predicci√≥n
        """
        # Verificar disponibilidad de modelos
        if not self.is_trained:
            print("‚ö†Ô∏è Modelos no entrenados, intentando cargar...")
            if not self.load_models():
                print("‚ùå Modelos no disponibles, usando detecci√≥n b√°sica")
                is_basic_anomaly = humidity_pct > self.threshold_basic
                context_info = {
                    'method': 'basic_threshold',
                    'threshold': self.threshold_basic,
                    'features_used': 1
                }
                return is_basic_anomaly, "Umbral b√°sico (50%)", 0.6, 0.0, context_info
        
        # Calcular caracter√≠sticas contextuales autom√°ticamente
        now = datetime.now()
        
        # Caracter√≠sticas temporales
        if hour is None:
            hour = now.hour
        if minute is None:
            minute = now.minute
        if day_of_week is None:
            day_of_week = now.weekday()  # 0=Lunes, 6=Domingo
        if is_weekend is None:
            is_weekend = 1 if day_of_week >= 5 else 0  # S√°bado=5, Domingo=6
        if is_night is None:
            is_night = 1 if hour < 6 or hour > 22 else 0  # 22:00 - 06:00
        
        # Estimar caracter√≠sticas avanzadas basadas en valores actuales
        # (En un sistema real, estas vendr√≠an de c√°lculos hist√≥ricos)
        
        # 1. Normalizar valor raw (estimaci√≥n basada en rango t√≠pico 0-1024)
        raw_normalized = min(max(raw_value / 1024.0, 0), 1)
        
        # 2. Categor√≠a de humedad basada en rangos est√°ndar
        if humidity_pct < 25:
            humidity_category = 0  # Baja
        elif humidity_pct < 60:
            humidity_category = 1  # Media
        else:
            humidity_category = 2  # Alta
        
        # 3. Nivel de riesgo de humedad (funci√≥n escalada)
        if humidity_pct < 20:
            humidity_risk_level = 0.1
        elif humidity_pct < 40:
            humidity_risk_level = 0.3
        elif humidity_pct < 60:
            humidity_risk_level = 0.6
        else:
            humidity_risk_level = 0.8
        
        # 4. Estabilidad del sensor (simulada como alta por defecto)
        sensor_stability = 1.0  # En sistema real, se calcular√≠a de lecturas recientes
        
        # 5. Cambios en humedad y raw (estimados como promedio para nueva lectura)
        humidity_change = 2.5  # Promedio t√≠pico del dataset
        raw_change = 10.0      # Promedio t√≠pico del dataset
        
        # Crear vector de caracter√≠sticas completo
        features = np.array([[
            humidity_pct,           # 0
            raw_value,             # 1  
            raw_normalized,        # 2
            hour,                  # 3
            day_of_week,          # 4
            is_weekend,           # 5
            is_night,             # 6
            humidity_category,     # 7
            humidity_risk_level,   # 8
            sensor_stability,      # 9
            humidity_change,       # 10
            raw_change            # 11
        ]])
        
        # Aplicar normalizaci√≥n entrenada
        features_scaled = self.scaler.transform(features)
        
        # ============= PREDICCI√ìN CON MODELO SUPERVISADO =============
        # Random Forest da probabilidades de clase
        prob_anomaly = self.model.predict_proba(features_scaled)[0][1]  # Probabilidad de anomal√≠a
        is_anomaly_classifier = self.model.predict(features_scaled)[0]   # Predicci√≥n binaria
        
        # ============= DETECCI√ìN CON MODELO NO SUPERVISADO =============
        # Isolation Forest da score de anomal√≠a y predicci√≥n binaria
        anomaly_score = self.anomaly_detector.decision_function(features_scaled)[0]
        is_anomaly_detector = self.anomaly_detector.predict(features_scaled)[0] == -1
        
        # ============= L√ìGICA DE CONSENSO INTELIGENTE MEJORADA =============
        confidence = prob_anomaly  # Confianza base del clasificador
        is_anomaly = False
        method = "ML Consenso Avanzado"
        
        # Ajustes contextuales de confianza
        confidence_boost = 0.0
        
        # Boost por contexto temporal riesgoso
        if is_night:
            confidence_boost += 0.05  # Noches m√°s problem√°ticas
        if is_weekend:
            confidence_boost += 0.03  # Fines de semana menos monitoreados
        
        # Boost por niveles de riesgo altos
        if humidity_risk_level > 0.6:
            confidence_boost += 0.1
        
        # Boost por estabilidad baja del sensor
        if sensor_stability < 0.8:
            confidence_boost += 0.05
        
        if is_anomaly_classifier and is_anomaly_detector:
            # Ambos detectan anomal√≠a ‚Üí M√ÅXIMA CONFIANZA
            is_anomaly = True
            method = "ML Alto Riesgo (Consenso Doble)"
            confidence = min(prob_anomaly + 0.2 + confidence_boost, 1.0)
            
        elif is_anomaly_classifier:
            # Solo supervisado detecta ‚Üí CONFIANZA MEDIA-ALTA
            is_anomaly = True
            method = "ML Clasificador (RF)"
            confidence = min(prob_anomaly + confidence_boost, 1.0)
            
        elif is_anomaly_detector:
            # Solo no supervisado detecta ‚Üí CONFIANZA MEDIA
            is_anomaly = True
            method = "ML Detector Anomal√≠as (IF)"
            confidence = min(0.7 + confidence_boost, 1.0)
            
        else:
            # Ninguno detecta ‚Üí NORMAL
            is_anomaly = False
            method = "ML Normal"
            confidence = max(1.0 - prob_anomaly - confidence_boost/2, 0.0)
        
        # Informaci√≥n contextual para debugging y an√°lisis
        context_info = {
            'features_used': len(self.feature_columns),
            'temporal_context': {
                'hour': hour,
                'day_of_week': day_of_week,
                'is_weekend': bool(is_weekend),
                'is_night': bool(is_night)
            },
            'risk_context': {
                'humidity_category': humidity_category,
                'humidity_risk_level': humidity_risk_level,
                'sensor_stability': sensor_stability
            },
            'ml_scores': {
                'rf_probability': prob_anomaly,
                'if_score': anomaly_score,
                'confidence_boost': confidence_boost
            },
            'predictions': {
                'rf_prediction': bool(is_anomaly_classifier),
                'if_prediction': bool(is_anomaly_detector)
            }
        }
        
        return is_anomaly, method, confidence, anomaly_score, context_info
    
    def get_risk_level(self, humidity_pct, confidence=0.5):
        """
        Determina el nivel de riesgo
        """
        if humidity_pct < 20:
            return "üü¢ BAJO", "Ambiente seco, sin riesgo"
        elif humidity_pct < 40:
            return "üü° NORMAL", "Humedad en rango normal"
        elif humidity_pct < 60:
            return "üü† ALTO", "Humedad elevada, monitorear"
        else:
            return "üî¥ CR√çTICO", "Posible filtraci√≥n detectada"
    
    def generate_alert_message(self, raw_value, humidity_pct):
        """
        Genera mensaje de alerta inteligente con an√°lisis ML contextualizado.
        
        Esta funci√≥n crea mensajes personalizados basados en:
        - Predicci√≥n ML (anomal√≠a detectada o no)
        - Nivel de confianza del algoritmo
        - Contexto de riesgo seg√∫n humedad
        - Recomendaciones espec√≠ficas para la situaci√≥n
        
        Los mensajes incluyen:
        - Emoji descriptivo seg√∫n severidad
        - Datos t√©cnicos del sensor
        - An√°lisis ML detallado (m√©todo y confianza)
        - Recomendaciones contextualizadas
        - Nivel de urgencia de la respuesta
        
        Args:
            raw_value (int): Valor crudo del sensor
            humidity_pct (float): Porcentaje de humedad
            
        Returns:
            tuple: (message, is_anomaly)
            - message (str): Mensaje formateado para WhatsApp
            - is_anomaly (bool): Indica si hay anomal√≠a detectada
        """
        # Ejecutar an√°lisis ML completo
        is_anomaly, method, confidence, anomaly_score = self.predict_anomaly(raw_value, humidity_pct)
        risk_level, risk_description = self.get_risk_level(humidity_pct, confidence)
        
        if is_anomaly:
            # ============= MENSAJE DE ALERTA =============
            message = f"üö® ALERTA DE FILTRACI√ìN DETECTADA\n"
            message += f"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n"
            
            # Datos t√©cnicos del sensor
            message += f"üìä DATOS DEL SENSOR:\n"
            message += f"   ‚Ä¢ Humedad: {humidity_pct:.1f}%\n"
            message += f"   ‚Ä¢ Valor raw: {raw_value}\n"
            message += f"   ‚Ä¢ Timestamp: {datetime.now().strftime('%H:%M:%S')}\n"
            message += f"   ‚Ä¢ Nivel de riesgo: {risk_level}\n\n"
            
            # An√°lisis de Machine Learning
            message += f"üß† AN√ÅLISIS INTELIGENTE:\n"
            message += f"   ‚Ä¢ M√©todo detecci√≥n: {method}\n"
            message += f"   ‚Ä¢ Confianza ML: {confidence:.1%}\n"
            message += f"   ‚Ä¢ Score anomal√≠a: {anomaly_score:.3f}\n"
            
            # Interpretaci√≥n del score
            if anomaly_score < -0.3:
                score_interpretation = "Muy an√≥malo"
            elif anomaly_score < -0.1:
                score_interpretation = "Moderadamente an√≥malo"
            else:
                score_interpretation = "Ligeramente an√≥malo"
            message += f"   ‚Ä¢ Interpretaci√≥n: {score_interpretation}\n\n"
            
            # Recomendaciones contextualizadas
            message += f"üí° RECOMENDACI√ìN:\n"
            message += f"   {risk_description}\n\n"
            
            # Nivel de urgencia
            if confidence > 0.8:
                urgency = "üî¥ URGENTE - Revisar inmediatamente"
            elif confidence > 0.6:
                urgency = "ÔøΩ MODERADO - Revisar en las pr√≥ximas horas"
            else:
                urgency = "üü° PRECAUCI√ìN - Monitorear de cerca"
                
            message += f"‚ö° URGENCIA: {urgency}\n"
            message += f"üîß Inspeccionar zona del sensor ahora"
            
        else:
            # ============= MENSAJE DE ESTADO NORMAL =============
            message = f"‚úÖ SISTEMA DRYWALL NORMAL\n"
            message += f"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            message += f"üìä Humedad: {humidity_pct:.1f}% | Raw: {raw_value}\n"
            message += f"üß† An√°lisis ML: {method}\n"
            message += f"üéØ Confianza: {confidence:.1%}\n"
            message += f"‚è∞ {datetime.now().strftime('%H:%M:%S')}"
            
        return message, is_anomaly
    
    def continuous_monitoring(self, get_sensor_data_func, send_message_func):
        """
        Monitoreo continuo con ML integrado
        """
        print("üîÑ Iniciando monitoreo inteligente...")
        
        last_alert_time = 0
        alert_cooldown = 300  # 5 minutos entre alertas
        
        while True:
            try:
                # Obtener datos del sensor
                data = get_sensor_data_func()
                if data is None:
                    continue
                    
                raw_value = data.get('raw', 0)
                humidity_pct = data.get('pct', 0)
                
                # Generar mensaje usando ML
                message, is_anomaly = self.generate_alert_message(raw_value, humidity_pct)
                
                # Enviar alerta si es necesario
                current_time = datetime.now().timestamp()
                if is_anomaly and (current_time - last_alert_time) > alert_cooldown:
                    send_message_func(message)
                    last_alert_time = current_time
                    print(f"üö® Alerta enviada: {humidity_pct}%")
                else:
                    print(f"üìä Normal: {humidity_pct}% (ML: {message.split('ML: ')[1] if 'ML: ' in message else 'N/A'})")
                
                time.sleep(10)  # Esperar 10 segundos
                
            except Exception as e:
                print(f"‚ùå Error en monitoreo: {e}")
                time.sleep(5)

def integrate_with_existing_bot():
    """
    Funci√≥n para integrar con el bot de WhatsApp existente
    """
    print("üîó Integrando ML con sistema existente...")
    
    # Importar funciones del sistema existente
    try:
        from lector_compartido import obtener_datos
        from main_bot import enviar_mensaje
        detector = SmartDryWallDetector()
        
        # Entrenar modelos si no existen
        if not detector.load_models():
            detector.train_models()
        
        # Funci√≥n wrapper para obtener datos
        def get_sensor_data():
            return obtener_datos()
        
        # Funci√≥n wrapper para enviar mensaje
        def send_alert(message):
            numero_usuario = os.getenv("USER_NUMBER")
            if numero_usuario:
                enviar_mensaje(message, numero_usuario)
            else:
                print("üì± Mensaje de alerta:", message)
        
        # Iniciar monitoreo inteligente
        detector.continuous_monitoring(get_sensor_data, send_alert)
        
    except ImportError as e:
        print(f"‚ö†Ô∏è No se pudo importar m√≥dulos existentes: {e}")
        print("üí° Ejecutar desde el directorio del proyecto principal")

def demo_ml_system():
    """
    Demostraci√≥n del sistema ML
    """
    print("üéØ DEMO - Sistema ML para DryWall Alert")
    print("=" * 50)
    
    detector = SmartDryWallDetector()
    
    # Entrenar modelos
    detector.train_models()
    
    # Casos de prueba
    test_cases = [
        {"raw": 300, "humidity": 15, "description": "Ambiente seco"},
        {"raw": 400, "humidity": 35, "description": "Humedad normal"},
        {"raw": 500, "humidity": 55, "description": "Humedad alta"},
        {"raw": 325, "humidity": 65, "description": "Posible filtraci√≥n"},
        {"raw": 200, "humidity": 80, "description": "Filtraci√≥n severa"}
    ]
    
    print("\nüß™ Probando casos de ejemplo:")
    for i, case in enumerate(test_cases, 1):
        print(f"\n--- Caso {i}: {case['description']} ---")
        message, is_anomaly = detector.generate_alert_message(case['raw'], case['humidity'])
        print(message)
        print(f"üéØ Anomal√≠a detectada: {'S√ç' if is_anomaly else 'NO'}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "demo":
        demo_ml_system()
    elif len(sys.argv) > 1 and sys.argv[1] == "integrate":
        integrate_with_existing_bot()
    else:
        print("üè† DRYWALL ALERT - Sistema ML Integrado")
        print("\nUso:")
        print("  python integrated_ml_system.py demo      # Ejecutar demostraci√≥n")
        print("  python integrated_ml_system.py integrate # Integrar con sistema existente")
        demo_ml_system()
